"""
Module ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng h·ªá th·ªëng chatbot tri·∫øt h·ªçc
"""

import logging
import json
from typing import Dict, List, Tuple
from datetime import datetime
import pandas as pd
from pathlib import Path
import time

from config import PHILOSOPHY_SCHOOLS, EVALUATION_QUESTIONS, LOG_DIR
from socratic_wrapper import SocraticChatbot

# C·∫•u h√¨nh logging cho evaluation
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_DIR / 'evaluation.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class ChatbotEvaluator:
    """
    L·ªõp ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng chatbot tri·∫øt h·ªçc
    """
    
    def __init__(self):
        """
        Kh·ªüi t·∫°o evaluator
        """
        self.results = {}
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
    def evaluate_single_chatbot(
        self, 
        school: str,
        test_questions: List[str] = None,
        use_socratic: bool = True
    ) -> Dict:
        """
        ƒê√°nh gi√° m·ªôt chatbot c·ªßa tr∆∞·ªùng ph√°i
        
        Args:
            school: T√™n tr∆∞·ªùng ph√°i
            test_questions: Danh s√°ch c√¢u h·ªèi test
            use_socratic: S·ª≠ d·ª•ng ph∆∞∆°ng ph√°p Socrates
            
        Returns:
            Dictionary ch·ª©a k·∫øt qu·∫£ ƒë√°nh gi√°
        """
        logger.info(f"\n{'='*60}")
        logger.info(f"ƒê√°nh gi√° chatbot: {PHILOSOPHY_SCHOOLS[school]['name']}")
        logger.info(f"{'='*60}")
        
        # Kh·ªüi t·∫°o chatbot
        try:
            chatbot = SocraticChatbot(school, use_gpu=True)
        except Exception as e:
            logger.error(f"Kh√¥ng th·ªÉ kh·ªüi t·∫°o chatbot {school}: {e}")
            return {
                "school": school,
                "status": "failed",
                "error": str(e)
            }
        
        # L·∫•y c√¢u h·ªèi test
        if test_questions is None:
            test_questions = EVALUATION_QUESTIONS.get(school, [])
        
        # Th√™m c√¢u h·ªèi ngo√†i ph·∫°m vi
        out_of_scope_questions = [
            "L√†m sao ƒë·ªÉ l·∫≠p tr√¨nh Python?",
            "C√¥ng th·ª©c h√≥a h·ªçc c·ªßa n∆∞·ªõc l√† g√¨?",
            "Ai l√† t·ªïng th·ªëng M·ªπ hi·ªán t·∫°i?"
        ]
        
        # K·∫øt qu·∫£ ƒë√°nh gi√°
        evaluation_results = {
            "school": school,
            "school_name": PHILOSOPHY_SCHOOLS[school]["name"],
            "total_questions": len(test_questions) + len(out_of_scope_questions),
            "in_scope_questions": len(test_questions),
            "out_of_scope_questions": len(out_of_scope_questions),
            "responses": [],
            "metrics": {}
        }
        
        # Test v·ªõi c√¢u h·ªèi trong ph·∫°m vi
        logger.info("\n--- Test c√¢u h·ªèi trong ph·∫°m vi ---")
        in_scope_results = []
        
        for i, question in enumerate(test_questions, 1):
            logger.info(f"\nC√¢u h·ªèi {i}: {question}")
            
            start_time = time.time()
            result = chatbot.chat(question, use_socratic=use_socratic)
            response_time = time.time() - start_time
            
            # ƒê√°nh gi√° response
            has_sources = len(result.get("sources", [])) > 0
            is_socratic = result.get("type") == "socratic"
            confidence = result.get("confidence", 0.0)
            
            logger.info(f"Lo·∫°i response: {result['type']}")
            logger.info(f"ƒê·ªô tin c·∫≠y: {confidence:.2%}")
            logger.info(f"C√≥ ngu·ªìn: {has_sources}")
            logger.info(f"Th·ªùi gian: {response_time:.2f}s")
            
            in_scope_results.append({
                "question": question,
                "response": result["response"][:200] + "...",
                "type": result["type"],
                "has_sources": has_sources,
                "confidence": confidence,
                "response_time": response_time,
                "is_correct": result["type"] != "rejection"
            })
        
        # Test v·ªõi c√¢u h·ªèi ngo√†i ph·∫°m vi
        logger.info("\n--- Test c√¢u h·ªèi ngo√†i ph·∫°m vi ---")
        out_scope_results = []
        
        for i, question in enumerate(out_of_scope_questions, 1):
            logger.info(f"\nC√¢u h·ªèi ngo√†i ph·∫°m vi {i}: {question}")
            
            start_time = time.time()
            result = chatbot.chat(question, use_socratic=use_socratic)
            response_time = time.time() - start_time
            
            # Ki·ªÉm tra c√≥ t·ª´ ch·ªëi ƒë√∫ng kh√¥ng
            is_rejection = result.get("type") == "rejection"
            
            logger.info(f"ƒê√£ t·ª´ ch·ªëi: {is_rejection}")
            logger.info(f"Th·ªùi gian: {response_time:.2f}s")
            
            out_scope_results.append({
                "question": question,
                "response": result["response"][:200] + "...",
                "type": result["type"],
                "is_correct_rejection": is_rejection,
                "response_time": response_time
            })
        
        # T√≠nh metrics
        evaluation_results["responses"] = {
            "in_scope": in_scope_results,
            "out_of_scope": out_scope_results
        }
        
        # Metrics cho c√¢u h·ªèi trong ph·∫°m vi
        correct_in_scope = sum(1 for r in in_scope_results if r["is_correct"])
        with_sources = sum(1 for r in in_scope_results if r["has_sources"])
        socratic_responses = sum(1 for r in in_scope_results if r["type"] == "socratic")
        avg_confidence = sum(r["confidence"] for r in in_scope_results) / max(1, len(in_scope_results))
        avg_response_time_in = sum(r["response_time"] for r in in_scope_results) / max(1, len(in_scope_results))
        
        # Metrics cho c√¢u h·ªèi ngo√†i ph·∫°m vi
        correct_rejections = sum(1 for r in out_scope_results if r["is_correct_rejection"])
        avg_response_time_out = sum(r["response_time"] for r in out_scope_results) / max(1, len(out_scope_results))
        
        evaluation_results["metrics"] = {
            "accuracy_in_scope": correct_in_scope / max(1, len(in_scope_results)),
            "source_citation_rate": with_sources / max(1, len(in_scope_results)),
            "socratic_rate": socratic_responses / max(1, len(in_scope_results)),
            "avg_confidence": avg_confidence,
            "rejection_accuracy": correct_rejections / max(1, len(out_scope_results)),
            "avg_response_time_in_scope": avg_response_time_in,
            "avg_response_time_out_scope": avg_response_time_out,
            "overall_accuracy": (correct_in_scope + correct_rejections) / (len(in_scope_results) + len(out_scope_results))
        }
        
        # Log t·ªïng k·∫øt
        logger.info(f"\n--- K·∫æT QU·∫¢ ƒê√ÅNH GI√Å {PHILOSOPHY_SCHOOLS[school]['name']} ---")
        logger.info(f"ƒê·ªô ch√≠nh x√°c trong ph·∫°m vi: {evaluation_results['metrics']['accuracy_in_scope']:.1%}")
        logger.info(f"T·ª∑ l·ªá tr√≠ch d·∫´n ngu·ªìn: {evaluation_results['metrics']['source_citation_rate']:.1%}")
        logger.info(f"T·ª∑ l·ªá ph·∫£n h·ªìi Socrates: {evaluation_results['metrics']['socratic_rate']:.1%}")
        logger.info(f"ƒê·ªô tin c·∫≠y trung b√¨nh: {evaluation_results['metrics']['avg_confidence']:.1%}")
        logger.info(f"ƒê·ªô ch√≠nh x√°c t·ª´ ch·ªëi: {evaluation_results['metrics']['rejection_accuracy']:.1%}")
        logger.info(f"Th·ªùi gian ph·∫£n h·ªìi TB: {evaluation_results['metrics']['avg_response_time_in_scope']:.2f}s")
        logger.info(f"ƒê·ªô ch√≠nh x√°c t·ªïng th·ªÉ: {evaluation_results['metrics']['overall_accuracy']:.1%}")
        
        # L·∫•y th·ªëng k√™ t·ª´ chatbot
        chatbot_stats = chatbot.get_stats()
        evaluation_results["chatbot_stats"] = chatbot_stats
        
        return evaluation_results
    
    def evaluate_all_chatbots(self, use_socratic: bool = True) -> Dict:
        """
        ƒê√°nh gi√° t·∫•t c·∫£ c√°c chatbot
        
        Args:
            use_socratic: S·ª≠ d·ª•ng ph∆∞∆°ng ph√°p Socrates
            
        Returns:
            Dictionary ch·ª©a k·∫øt qu·∫£ ƒë√°nh gi√° t·ªïng h·ª£p
        """
        logger.info("\n" + "="*70)
        logger.info("B·∫ÆT ƒê·∫¶U ƒê√ÅNH GI√Å TO√ÄN B·ªò H·ªÜ TH·ªêNG CHATBOT TRI·∫æT H·ªåC")
        logger.info("="*70)
        
        all_results = {
            "timestamp": self.timestamp,
            "use_socratic": use_socratic,
            "schools": {},
            "summary": {}
        }
        
        # ƒê√°nh gi√° t·ª´ng tr∆∞·ªùng ph√°i
        for school in PHILOSOPHY_SCHOOLS.keys():
            try:
                result = self.evaluate_single_chatbot(school, use_socratic=use_socratic)
                all_results["schools"][school] = result
                time.sleep(1)  # Tr√°nh overload
            except Exception as e:
                logger.error(f"L·ªói ƒë√°nh gi√° {school}: {e}")
                all_results["schools"][school] = {
                    "school": school,
                    "status": "error",
                    "error": str(e)
                }
        
        # T√≠nh metrics t·ªïng h·ª£p
        successful_schools = [
            school_data for school_data in all_results["schools"].values()
            if "metrics" in school_data
        ]
        
        if successful_schools:
            all_results["summary"] = {
                "total_schools": len(PHILOSOPHY_SCHOOLS),
                "successful_evaluations": len(successful_schools),
                "avg_accuracy_in_scope": sum(s["metrics"]["accuracy_in_scope"] for s in successful_schools) / len(successful_schools),
                "avg_source_citation_rate": sum(s["metrics"]["source_citation_rate"] for s in successful_schools) / len(successful_schools),
                "avg_socratic_rate": sum(s["metrics"]["socratic_rate"] for s in successful_schools) / len(successful_schools),
                "avg_rejection_accuracy": sum(s["metrics"]["rejection_accuracy"] for s in successful_schools) / len(successful_schools),
                "avg_confidence": sum(s["metrics"]["avg_confidence"] for s in successful_schools) / len(successful_schools),
                "avg_response_time": sum(s["metrics"]["avg_response_time_in_scope"] for s in successful_schools) / len(successful_schools),
                "avg_overall_accuracy": sum(s["metrics"]["overall_accuracy"] for s in successful_schools) / len(successful_schools)
            }
        
        # L∆∞u k·∫øt qu·∫£
        self.save_results(all_results)
        
        # In b√°o c√°o t·ªïng k·∫øt
        self.print_summary_report(all_results)
        
        return all_results
    
    def save_results(self, results: Dict):
        """
        L∆∞u k·∫øt qu·∫£ ƒë√°nh gi√°
        
        Args:
            results: K·∫øt qu·∫£ ƒë√°nh gi√°
        """
        # L∆∞u JSON
        json_path = LOG_DIR / f"evaluation_{self.timestamp}.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        logger.info(f"\nƒê√£ l∆∞u k·∫øt qu·∫£ JSON: {json_path}")
        
        # L∆∞u CSV cho metrics
        if results.get("summary"):
            csv_data = []
            for school, data in results["schools"].items():
                if "metrics" in data:
                    row = {"school": PHILOSOPHY_SCHOOLS[school]["name"]}
                    row.update(data["metrics"])
                    csv_data.append(row)
            
            if csv_data:
                df = pd.DataFrame(csv_data)
                csv_path = LOG_DIR / f"evaluation_metrics_{self.timestamp}.csv"
                df.to_csv(csv_path, index=False, encoding='utf-8')
                logger.info(f"ƒê√£ l∆∞u metrics CSV: {csv_path}")
    
    def print_summary_report(self, results: Dict):
        """
        In b√°o c√°o t·ªïng k·∫øt
        
        Args:
            results: K·∫øt qu·∫£ ƒë√°nh gi√°
        """
        print("\n" + "="*70)
        print("B√ÅO C√ÅO T·ªîNG K·∫æT ƒê√ÅNH GI√Å H·ªÜ TH·ªêNG")
        print("="*70)
        
        if results.get("summary"):
            summary = results["summary"]
            print(f"\nüìä TH·ªêNG K√ä T·ªîNG H·ª¢P:")
            print(f"  ‚Ä¢ S·ªë tr∆∞·ªùng ph√°i ƒë√°nh gi√°: {summary['successful_evaluations']}/{summary['total_schools']}")
            print(f"  ‚Ä¢ ƒê·ªô ch√≠nh x√°c t·ªïng th·ªÉ: {summary['avg_overall_accuracy']:.1%}")
            print(f"  ‚Ä¢ ƒê·ªô ch√≠nh x√°c trong ph·∫°m vi: {summary['avg_accuracy_in_scope']:.1%}")
            print(f"  ‚Ä¢ ƒê·ªô ch√≠nh x√°c t·ª´ ch·ªëi: {summary['avg_rejection_accuracy']:.1%}")
            print(f"  ‚Ä¢ T·ª∑ l·ªá tr√≠ch d·∫´n ngu·ªìn: {summary['avg_source_citation_rate']:.1%}")
            print(f"  ‚Ä¢ T·ª∑ l·ªá s·ª≠ d·ª•ng Socrates: {summary['avg_socratic_rate']:.1%}")
            print(f"  ‚Ä¢ ƒê·ªô tin c·∫≠y trung b√¨nh: {summary['avg_confidence']:.1%}")
            print(f"  ‚Ä¢ Th·ªùi gian ph·∫£n h·ªìi TB: {summary['avg_response_time']:.2f}s")
        
        print("\nüìã CHI TI·∫æT T·ª™NG TR∆Ø·ªúNG PH√ÅI:")
        for school, data in results["schools"].items():
            print(f"\n  {PHILOSOPHY_SCHOOLS[school]['name']}:")
            if "metrics" in data:
                print(f"    ‚úì ƒê·ªô ch√≠nh x√°c: {data['metrics']['overall_accuracy']:.1%}")
                print(f"    ‚úì T·ª∑ l·ªá t·ª´ ch·ªëi ƒë√∫ng: {data['metrics']['rejection_accuracy']:.1%}")
                print(f"    ‚úì Th·ªùi gian TB: {data['metrics']['avg_response_time_in_scope']:.2f}s")
            else:
                print(f"    ‚úó L·ªói: {data.get('error', 'Unknown')}")
        
        print("\n" + "="*70)


def run_full_evaluation():
    """
    Ch·∫°y ƒë√°nh gi√° ƒë·∫ßy ƒë·ªß cho to√†n b·ªô h·ªá th·ªëng
    """
    evaluator = ChatbotEvaluator()
    
    # ƒê√°nh gi√° v·ªõi ph∆∞∆°ng ph√°p Socrates
    print("\nüîç ƒê√ÅNH GI√Å V·ªöI PH∆Ø∆†NG PH√ÅP SOCRATES")
    results_socratic = evaluator.evaluate_all_chatbots(use_socratic=True)
    
    # ƒê√°nh gi√° kh√¥ng d√πng Socrates (ƒë·ªÉ so s√°nh)
    evaluator.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    print("\n\nüîç ƒê√ÅNH GI√Å KH√îNG D√ôNG SOCRATES (ƒê·ªÇ SO S√ÅNH)")
    results_direct = evaluator.evaluate_all_chatbots(use_socratic=False)
    
    # So s√°nh k·∫øt qu·∫£
    print("\n" + "="*70)
    print("SO S√ÅNH PH∆Ø∆†NG PH√ÅP SOCRATES VS TR·ª∞C TI·∫æP")
    print("="*70)
    
    if results_socratic.get("summary") and results_direct.get("summary"):
        print("\nüìä V·ªõi Socrates:")
        print(f"  ‚Ä¢ ƒê·ªô tin c·∫≠y TB: {results_socratic['summary']['avg_confidence']:.1%}")
        print(f"  ‚Ä¢ T·ª∑ l·ªá Socratic: {results_socratic['summary']['avg_socratic_rate']:.1%}")
        
        print("\nüìä Kh√¥ng Socrates:")
        print(f"  ‚Ä¢ ƒê·ªô tin c·∫≠y TB: {results_direct['summary']['avg_confidence']:.1%}")
        print(f"  ‚Ä¢ T·ª∑ l·ªá Socratic: {results_direct['summary']['avg_socratic_rate']:.1%}")
    
    print("\n‚úÖ Ho√†n t·∫•t ƒë√°nh gi√° h·ªá th·ªëng!")
    print(f"üìÅ K·∫øt qu·∫£ ƒë∆∞·ª£c l∆∞u t·∫°i: {LOG_DIR}")


def run_single_school_test(school: str = "stoicism"):
    """
    Test ƒë√°nh gi√° cho m·ªôt tr∆∞·ªùng ph√°i c·ª• th·ªÉ
    
    Args:
        school: T√™n tr∆∞·ªùng ph√°i c·∫ßn test
    """
    print(f"\nüß™ TEST ƒê√ÅNH GI√Å CHO {PHILOSOPHY_SCHOOLS[school]['name'].upper()}")
    print("="*60)
    
    evaluator = ChatbotEvaluator()
    
    # C√¢u h·ªèi test t√πy ch·ªânh
    custom_questions = [
        "L√†m th·∫ø n√†o ƒë·ªÉ s·ªëng t·ªët?",
        "√ù nghƒ©a c·ªßa cu·ªôc s·ªëng l√† g√¨?",
        "Con ng∆∞·ªùi c√≥ t·ª± do kh√¥ng?"
    ]
    
    result = evaluator.evaluate_single_chatbot(
        school,
        test_questions=custom_questions,
        use_socratic=True
    )
    
    # In k·∫øt qu·∫£ chi ti·∫øt
    if "metrics" in result:
        print("\nüìà K·∫æT QU·∫¢ METRICS:")
        for metric, value in result["metrics"].items():
            if isinstance(value, float):
                if "time" in metric:
                    print(f"  ‚Ä¢ {metric}: {value:.2f}s")
                else:
                    print(f"  ‚Ä¢ {metric}: {value:.1%}")
    
    print("\n‚úÖ Ho√†n t·∫•t test!")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        if sys.argv[1] == "full":
            # Ch·∫°y ƒë√°nh gi√° ƒë·∫ßy ƒë·ªß
            run_full_evaluation()
        elif sys.argv[1] in PHILOSOPHY_SCHOOLS:
            # Test m·ªôt tr∆∞·ªùng ph√°i c·ª• th·ªÉ
            run_single_school_test(sys.argv[1])
        else:
            print(f"L·ªánh kh√¥ng h·ª£p l·ªá: {sys.argv[1]}")
            print(f"S·ª≠ d·ª•ng: python evaluation.py [full|{' | '.join(PHILOSOPHY_SCHOOLS.keys())}]")
    else:
        # M·∫∑c ƒë·ªãnh ch·∫°y ƒë√°nh gi√° ƒë·∫ßy ƒë·ªß
        run_full_evaluation()